{"name":"Scalawebscraper","tagline":"Scala Webscraper","body":"Scala Webscraper\r\n================\r\n\r\n## Getting started\r\n\r\nThe project is build with Scala 2.10.2 and sbt 0.13.0, both can be installed\r\nusing this [install script](https://gist.github.com/Rovak/4967148)\r\n\r\nTo try the example navigate to the project folder and run `sbt \"project scraper-demo\" run`\r\nwhich will start the example scraper\r\n\r\n## DSL\r\n\r\nThe webscraper provides a simple DSL to write scrape rules\r\n\r\n```scala\r\nimport org.rovak.scraper.ScrapeManager._\r\n\r\nobject Google {\r\n  val results = \"#res li.g h3.r a\"\r\n  def search(term: String) = {\r\n    \"http://www.google.com/search?q=\" + term.replace(\" \", \"+\")\r\n  }\r\n}\r\n\r\n// Open the search results page for the query \"php elephant\"\r\nscrape from Google.search(\"php elephant\") open { implicit page =>\r\n\r\n  // Iterate through every result link\r\n  Google.results each { x: Element =>\r\n  \r\n    val link = x.select(\"a[href]\").attr(\"abs:href\").substring(28)\r\n    if (link.isValidURL) {\r\n\r\n      // Iterate through every found link in the found page\r\n      scrape from link each (x => println(\"found: \" + x))\r\n    }\r\n  }\r\n}\r\n\r\n```\r\n\r\n## Spiders\r\n\r\nA spider is a scraper which recursively loads a page and opens every link it finds. It will\r\nkeep scraping until all pages within the allowed domains are visited once.\r\n\r\nThe following snippet demonstrates a basic spider which crawls a website and provides\r\nhooks to do something with the data\r\n\r\n```scala\r\nnew Spider {\r\n  startUrls ::= \"http://events.stanford.edu/\"\r\n  allowedDomains ::= \"events.stanford.edu\"\r\n\r\n  onReceivedPage ::= { page: WebPage =>\r\n    // Page received\r\n  }\r\n\r\n  onLinkFound ::= { link: Href =>\r\n    println(s\"Found link ${link.url} with name ${link.name}\")\r\n  }\r\n}.start()\r\n```\r\n\r\nThe spider can be extended by providing traits, if you want to scrape emails then\r\nthe EmailSpider trait can be used.\r\n\r\n```scala\r\nnew Spider with EmailSpider {\r\n  startUrls ::= \"http://events.stanford.edu/\"\r\n  allowedDomains ::= \"events.stanford.edu\"\r\n\r\n  onEmailFound ::= { email: String =>\r\n    // Email found\r\n  }\r\n\r\n  onReceivedPage ::= { page: WebPage =>\r\n    // Page received\r\n  }\r\n\r\n  onLinkFound ::= { link: Href =>\r\n    println(s\"Found link ${link.url} with name ${link.name}\")\r\n  }\r\n}.start()\r\n```\r\n\r\nMultiple spiders can be mixed together\r\n\r\n```scala\r\nnew Spider with EmailSpider with SitemapSpider {\r\n  startUrls ::= \"http://events.stanford.edu/\"\r\n  allowedDomains ::= \"events.stanford.edu\"\r\n  sitemapUrls ::= \"http://events.stanford.edu/sitemap.xml\"\r\n\r\n  onEmailFound ::= { email: String =>\r\n    println(\"Found email: \" + email)\r\n  }\r\n\r\n  onReceivedPage ::= { page: WebPage =>\r\n    // Page received\r\n  }\r\n\r\n  onLinkFound ::= { link: Href =>\r\n    println(s\"Found link ${link.url} with name ${link.name}\")\r\n  }\r\n}.start()\r\n```\r\n\r\n## Documentation\r\n\r\n- [API](http://ci.razko.nl/job/WebsiteScraper/Documentation/index.html)\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}